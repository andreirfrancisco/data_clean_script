{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e639c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os \n",
    "folder = \"C:\\\\Users\\\\Andre\\\\Desktop\\\\update_weekly\"\n",
    "# COLLECTING ALL REPORTS \n",
    "def collect_files(folder, pattern):\n",
    "    dictionary = {}\n",
    "    for second_folder in os.listdir(folder):\n",
    "    # folder\n",
    "    # get the files and date\n",
    "        if '12_18_24' not in second_folder and  '12_25_24' not in second_folder:\n",
    "            second_path = os.path.join(folder, second_folder)\n",
    "            if '.py' not in second_path and '.ipynb' not in second_path and \"MERGED_\" not in second_path:\n",
    "                for file in os.listdir(second_path):\n",
    "                    if pattern in file.lower() and \"merged_\" not in file.lower():\n",
    "                        date = second_folder.replace(\"_Report\", \"\")\n",
    "                        date = date.replace(\"_\", \"/\")\n",
    "                        date = date + \"/\" + \"2025\"\n",
    "                        # print(date)\n",
    "                        dictionary[date] = os.path.join(second_folder,file)\n",
    "    return dictionary\n",
    "\n",
    "def sheets(path):\n",
    "    sheets = pd.ExcelFile(path).sheet_names\n",
    "    # print(sheets)\n",
    "    return sheets\n",
    "# Merging for all files\n",
    "def merge(folder):\n",
    "    ''''\n",
    "    Use main folder name like ARRIVALS\n",
    "    '''\n",
    "    # gather all files into dataframe\n",
    "    data_frames = []\n",
    "    for file in os.listdir(folder):\n",
    "        if \"_DATA.xlsx\" not in file:\n",
    "            df = pd.read_excel(os.path.join(folder, file))\n",
    "            df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "            post_columns = df.columns\n",
    "            data_frames.append(df)\n",
    "    # the dataset concat from all files\n",
    "    merged_df = pd.concat(data_frames, ignore_index=True)  # Use ignore_index=True to reset the index\n",
    "    # final name of the file\n",
    "    path = (os.path.basename(folder) + \"_DATA.xlsx\")\n",
    "    path = os.path.join(folder, path)\n",
    "    print(path)\n",
    "    # load file into folder\n",
    "    merged_df.to_excel(path)\n",
    "def preview(path):\n",
    "    data = pd.read_excel(path)\n",
    "    data = data.loc[:, ~data.columns.str.contains('^Unnamed')]\n",
    "    data['Week'].unique()\n",
    "    pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14a9b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_pattern ='Report'\n",
    "reports = []\n",
    "folder = \"C:\\\\Users\\\\Andre\\\\Desktop\\\\update_weekly\"\n",
    "for file in os.listdir(folder):\n",
    "    if file.endswith(common_pattern):\n",
    "        reports.append(file)\n",
    "for report in reports : \n",
    "    # find the index\n",
    "    rep_index = report.find(common_pattern)\n",
    "    date= report[0:rep_index]\n",
    "    if report.endswith(\"24_\") == False and '2025' not in report and '2024' not:\n",
    "       new_file_name = (date + \"2025\" + \"_Report\")\n",
    "       os.rename(report, new_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "596ae09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to collect files \n",
    "def collect_files(folder, pattern):\n",
    "    file_dictionary = {}\n",
    "    # list all folders\n",
    "    for file in os.listdir(folder):\n",
    "        # filter by Report\n",
    "        # print(file)\n",
    "        if file.endswith('Report'):\n",
    "            report_path = os.path.join(folder, file)\n",
    "            # print(report_path)\n",
    "            # extract date as key \n",
    "            rep_index = file.find(pattern)\n",
    "            key= file[0:(rep_index-(len('Report')))]\n",
    "            key = key.replace(\"_\", \"/\")\n",
    "            # print(key)\n",
    "                # filter by Pattern\n",
    "            for file_2 in os.listdir(report_path):\n",
    "                if pattern.lower() in file_2.lower():\n",
    "                    value = file_2\n",
    "                    # file_2 path \n",
    "                    file_2_path = os.path.join(report_path, file_2)\n",
    "                    value = file_2_path\n",
    "                        \n",
    "                    file_dictionary[key] = value\n",
    "    return file_dictionary\n",
    "# method to create dataset for each sheet\n",
    "\n",
    "def sheet_dataset(folder):\n",
    "    # takes the folder and insert file as FOLDER_dataset.xlsx\n",
    "    dfs = []\n",
    "    for file in os.listdir(folder):\n",
    "        if \"_dataset.xlsx\" not in file:\n",
    "            df = pd.read_excel(os.path.join(folder, file), index_col=0)\n",
    "            dfs.append(df)\n",
    "    merged_df = pd.concat(dfs, ignore_index=True)  # Use ignore_index=True to reset the index\n",
    "    path = (os.path.basename(folder) + \"_dataset.xlsx\")\n",
    "    path = os.path.join(folder, path)\n",
    "    return merged_df\n",
    "    # load file into folder\n",
    "# method to create dataset for each sheet\n",
    "\n",
    "def sheet_dataset_to_main_file(folder : list, main_file_name : str):\n",
    "    pass\n",
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9d3b9371",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9856917",
   "metadata": {},
   "source": [
    "# 1. Arrival Separations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f66ee2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each sheet create \n",
    "def Arrivals(file, date):\n",
    "    try:\n",
    "        df = pd.read_excel(file, sheet_name = \"Arrivals\")\n",
    "    # add week \n",
    "    except ValueError as e:\n",
    "        print(f\"An error occurred: {date}\")\n",
    "    df['week'] = date\n",
    "    date = date.replace(\"/\", \"_\")\n",
    "    sheet_name = date + \"_for_arrivals_report\" + \".xlsx\"\n",
    "    file_path = os.path.join(\"ARRIVALS\", sheet_name)\n",
    "    df.to_excel(file_path, index=False)\n",
    "def Separations(file , date):\n",
    "    try:\n",
    "        df = pd.read_excel(file, sheet_name = \"Separations\")\n",
    "    # add week \n",
    "    except ValueError as e:\n",
    "        print(f\"An error occurred: {date}\")\n",
    "    df['week'] = date\n",
    "    date = date.replace(\"/\", \"_\")\n",
    "    print(date)\n",
    "    sheet_name = date + \"_for_separations_report\" + \".xlsx\"\n",
    "    file_path = os.path.join(\"SEPARATIONS\", sheet_name)\n",
    "    df.to_excel(file_path, index=False)\n",
    "def Ordinary_Separations(file , date):\n",
    "    try:\n",
    "        df = pd.read_excel(file, sheet_name=\"Ordinary Separations\")\n",
    "        # Add your code that manipulates the DataFrame here\n",
    "    except ValueError as e:\n",
    "        print(f\"An error occurred: {date}\")\n",
    "    df['week'] = date\n",
    "    date = date.replace(\"/\", \"_\")\n",
    "    sheet_name = date + \"_for_ordinary_separations_report\" + \".xlsx\"\n",
    "    file_path = os.path.join(\"ORDINARY_SEPARATIONS\", sheet_name)\n",
    "    df.to_excel(file_path, index=False)\n",
    "\n",
    "def Resignations(file , date):\n",
    "    try:\n",
    "        df = pd.read_excel(file, sheet_name=\"Resignations\")\n",
    "        # Add your code that manipulates the DataFrame here\n",
    "    except ValueError as e:\n",
    "        print(f\"An error occurred: {date}\")\n",
    "    df['week'] = date\n",
    "    date = date.replace(\"/\", \"_\")\n",
    "    sheet_name = date + \"_for_resignations_report\" + \".xlsx\"\n",
    "    file_path = os.path.join(\"RESIGNATIONS\", sheet_name)\n",
    "    df.to_excel(file_path, index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e850334",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = 'arrivals_separations'\n",
    "arrival_files= collect_files(folder = folder, pattern = pattern)\n",
    "\n",
    "# for date , file  in arrival_files.items():\n",
    "#     Arrivals(file, date)\n",
    "#     Separations(file, date)\n",
    "#     Ordinary_Separations(file, date)\n",
    "#     Resignations(file, date)\n",
    "\n",
    "arr_sheet = sheet_dataset(\"ARRIVALS\")\n",
    "sep_sheet = sheet_dataset(\"SEPARATIONS\")\n",
    "ors_sheet = sheet_dataset(\"ORDINARY_SEPARATIONS\")\n",
    "res_sheet = sheet_dataset(\"RESIGNATIONS\")\n",
    "\n",
    "with pd.ExcelWriter('arrivals_separations_dataset.xlsx', engine='openpyxl') as writer:\n",
    "    arr_sheet.to_excel(writer, sheet_name='Arrivals', index=False)  # Save df to Sheet1\n",
    "    sep_sheet.to_excel(writer, sheet_name='Separations', index=False)  # Save df_2 to Sheet2\n",
    "    ors_sheet.to_excel(writer, sheet_name='Ordinary Separations', index=False)  # Save df to Sheet1\n",
    "    res_sheet.to_excel(writer, sheet_name='Resignations', index=False)  # Save df_2 to Sheet2\n",
    "\n",
    "sheets = pd.ExcelFile('arrivals_separations_dataset.xlsx').sheet_names\n",
    "\n",
    "sheets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d19b55f",
   "metadata": {},
   "source": [
    "# 2. Clothing Allowance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3590a691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# methods for cleaning and transforming data\n",
    "def Region_Summary(file,date):\n",
    "    sheet_name = 'Region Summary'\n",
    "    try : \n",
    "        df = pd.read_excel(file, sheet_name =sheet_name)\n",
    "    except ValueError as e:\n",
    "        print(f\"An error occurred: {date}\")\n",
    "    # remove the first column\n",
    "    df = df[1:]\n",
    "    df.columns = df.iloc[0]\n",
    "    # then remove the first row \n",
    "    df = df[1:]\n",
    "    df.columns\n",
    "    # rename columns\n",
    "    df = df.rename(columns = {df.columns[1]: \"Issue Amount\"})\n",
    "    df = df.rename(columns = {df.columns[2]: \"Spent Amount\"})\n",
    "    # add column\n",
    "    df['week'] = date\n",
    "    week = date.replace(\"/\", \"_\")\n",
    "    sheet_name = week + \"_for_region_summary_report\" + \".xlsx\"\n",
    "    file_path = os.path.join(\"CLOTHING_REGION_SUMMARY\", sheet_name)\n",
    "    df.to_excel(file_path, index=False)\n",
    "def Region_Center_Summary(file, date):\n",
    "    sheet_name = \"Region Center Summary\"\n",
    "    try : \n",
    "        df = pd.read_excel(file, sheet_name=sheet_name)\n",
    "    except ValueError as e:\n",
    "        print(f\"An error occurred: {date}\")\n",
    "    \n",
    "    # use first row as columns\n",
    "    df.columns = df.iloc[0]\n",
    "    df = df[1:]\n",
    "    df['week'] = date\n",
    "    week = date.replace(\"/\", \"_\")\n",
    "    sheet_name = week + \"_for_region_center_summary_report\" + \".xlsx\"\n",
    "    file_path = os.path.join(\"CLOTHING_REGION_CENTER_SUMMARY\", sheet_name)\n",
    "    df.to_excel(file_path, index=False)\n",
    "def Clothing_PY(file, date):\n",
    "    sheet_name = 'PY to Date'\n",
    "    try : \n",
    "        df = pd.read_excel(file, sheet_name = sheet_name)\n",
    "    except ValueError as e:\n",
    "        print(f\"An error occurred: {date}\")\n",
    "    df = df[1:]\n",
    "    df.columns = df.iloc[0]\n",
    "    df = df[1:]\n",
    "    df = df.rename(columns = {df.columns[1]: \"Issue Amount(PY)\"})\n",
    "    df = df.rename(columns = {df.columns[2]: \"Spent Amount(PY)\"})\n",
    "    df['week'] = date\n",
    "    week = date.replace(\"/\", \"_\")\n",
    "    sheet_name = week + \"_for_clothing_py_report\" + \".xlsx\"\n",
    "    file_path = os.path.join(\"CLOTHING_PY_SUMMARY\", sheet_name)\n",
    "    df.to_excel(file_path, index=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59b4061",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = 'clothing_allowance'\n",
    "clothing_files= collect_files(folder = folder, pattern = pattern)\n",
    "\n",
    "for date, file in clothing_files.items():\n",
    "    Region_Summary(file , date)\n",
    "    Region_Center_Summary(file, date)\n",
    "    Clothing_PY(file, date)\n",
    "\n",
    "\n",
    "clo_reg_sheet = sheet_dataset(\"CLOTHING_REGION_SUMMARY\")\n",
    "clo_reg_center_sheet = sheet_dataset(\"CLOTHING_REGION_CENTER_SUMMARY\")\n",
    "clo_py_sheet = sheet_dataset(\"CLOTHING_PY_SUMMARY\")\n",
    "\n",
    "\n",
    "with pd.ExcelWriter('clothing_allowance_dataset.xlsx', engine='openpyxl') as writer:\n",
    "    clo_py_sheet.to_excel(writer, sheet_name=\"PY\", index = False)\n",
    "    clo_reg_center_sheet.to_excel(writer, sheet_name = \"Region and Center\", index= False)\n",
    "    clo_reg_sheet.to_excel(writer, sheet_name = \"Region\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48994151",
   "metadata": {},
   "source": [
    "# 3. Online Prospect "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "6025e11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sheets(\"3_12_Report\\OnlineProspect_ContactStatusSummary_03-12-2025.xlsx\")\n",
    "def Online_prospect_week(file, date):\n",
    "    sheet_name = 'Week data'\n",
    "    try : \n",
    "        df = pd.read_excel(file, sheet_name = sheet_name)\n",
    "    except ValueError as e:\n",
    "        print(f\"An error occurred: {date}\")\n",
    "        sheet_name = 'Week'\n",
    "        df = pd.read_excel(file, sheet_name=sheet_name)\n",
    "    if 'reg_name' in df.columns:\n",
    "        df = df.rename(columns={'reg_name': 'Region'})\n",
    "    df['week'] = date\n",
    "    week = date.replace(\"/\", \"_\")\n",
    "    sheet_name = week + \"_for_online_prospect_report\" + \".xlsx\"\n",
    "    file_path = os.path.join(\"ONLINE_PROSPECT\", sheet_name)\n",
    "    df.to_excel(file_path, index=False)\n",
    "def Online_prospect_py(file, date):\n",
    "    sheet_name = 'PY'\n",
    "    try : \n",
    "        df = pd.read_excel(file, sheet_name = sheet_name)\n",
    "    except ValueError as e:\n",
    "        print(f\"An error occurred: {date}\")\n",
    "    df = df[2:]\n",
    "    df.columns = df.iloc[0]\n",
    "    df = df.reset_index()\n",
    "    df = df.drop(columns={'index'})\n",
    "    df = df[1:]\n",
    "    if 'region' in df.columns:\n",
    "        df = df.rename(columns={'region': 'Region'})\n",
    "    application_types = list(df.columns)\n",
    "    application_types = application_types.remove('Region')\n",
    "    # unpivot the tbale\n",
    "    df = df.melt(id_vars= 'Region', value_vars= application_types , var_name = \"Application Type\",  value_name='Count')\n",
    "    df = df.sort_values(by = 'Region')\n",
    "    df['week'] = date\n",
    "    week = date.replace(\"/\", \"_\")\n",
    "    sheet_name = week + \"_for_online_prospect_py_report\" + \".xlsx\"\n",
    "    file_path = os.path.join(\"ONLINE_PROSPECT_PY\", sheet_name)\n",
    "    df.to_excel(file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "5e3c142e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: 12/18/2024\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "Application Type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "week",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Region",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "f600bd1e-1aff-45a5-aa67-333fcb97c9f0",
       "rows": [
        [
         "Application Completed",
         "869",
         "12/18/2024",
         null
        ],
        [
         "Contacted-Not Eligible",
         "878",
         "12/18/2024",
         null
        ],
        [
         "Contacted-Message Left",
         "15280",
         "12/18/2024",
         null
        ],
        [
         "Contacted-Not Interested",
         "494",
         "12/18/2024",
         null
        ],
        [
         "Contacted-Interview Scheduled",
         "2679",
         "12/18/2024",
         null
        ],
        [
         "Unable to Contact-No Response",
         "1940",
         "12/18/2024",
         null
        ],
        [
         "New Prospect",
         "1873",
         "12/18/2024",
         null
        ],
        [
         "Contacted-Interested",
         "2487",
         "12/18/2024",
         null
        ],
        [
         "Grand Total",
         "28714",
         "12/18/2024",
         null
        ],
        [
         "Contacted-Referred to other AC",
         "490",
         "12/18/2024",
         null
        ],
        [
         "Contacted-Undecided",
         "143",
         "12/18/2024",
         null
        ],
        [
         "Duplicate Record",
         "939",
         "12/18/2024",
         null
        ],
        [
         "Application Started",
         "481",
         "12/18/2024",
         null
        ],
        [
         "Unable to Contact-Invalid Data",
         "161",
         "12/18/2024",
         null
        ],
        [
         "Contacted-Not Interested",
         "634",
         "12/18/2024",
         null
        ],
        [
         "Contacted-Interested",
         "1874",
         "12/18/2024",
         null
        ],
        [
         "Unable to Contact-Invalid Data",
         "109",
         "12/18/2024",
         null
        ],
        [
         "Grand Total",
         "21189",
         "12/18/2024",
         null
        ],
        [
         "Application Completed",
         "598",
         "12/18/2024",
         null
        ],
        [
         "Unable to Contact-No Response",
         "401",
         "12/18/2024",
         null
        ],
        [
         "Application Started",
         "256",
         "12/18/2024",
         null
        ],
        [
         "Duplicate Record",
         "4872",
         "12/18/2024",
         null
        ],
        [
         "Contacted-Interview Scheduled",
         "782",
         "12/18/2024",
         null
        ],
        [
         "Contacted-Referred to other AC",
         "577",
         "12/18/2024",
         null
        ],
        [
         "Contacted-Undecided",
         "184",
         "12/18/2024",
         null
        ],
        [
         "New Prospect",
         "1762",
         "12/18/2024",
         null
        ],
        [
         "Contacted-Not Eligible",
         "762",
         "12/18/2024",
         null
        ],
        [
         "Contacted-Message Left",
         "8378",
         "12/18/2024",
         null
        ],
        [
         "Contacted-Referred to other AC",
         "144",
         "12/18/2024",
         null
        ],
        [
         "Contacted-Not Eligible",
         "352",
         "12/18/2024",
         null
        ],
        [
         "New Prospect",
         "5468",
         "12/18/2024",
         null
        ],
        [
         "Contacted-Not Interested",
         "620",
         "12/18/2024",
         null
        ],
        [
         "Contacted-Undecided",
         "380",
         "12/18/2024",
         null
        ],
        [
         "Duplicate Record",
         "1893",
         "12/18/2024",
         null
        ],
        [
         "Contacted-Message Left",
         "11143",
         "12/18/2024",
         null
        ],
        [
         "Grand Total",
         "23052",
         "12/18/2024",
         null
        ],
        [
         "Unable to Contact-Invalid Data",
         "130",
         "12/18/2024",
         null
        ],
        [
         "Application Completed",
         "292",
         "12/18/2024",
         null
        ],
        [
         "Contacted-Interested",
         "1128",
         "12/18/2024",
         null
        ],
        [
         "Contacted-Interview Scheduled",
         "492",
         "12/18/2024",
         null
        ],
        [
         "Unable to Contact-No Response",
         "863",
         "12/18/2024",
         null
        ],
        [
         "Application Started",
         "147",
         "12/18/2024",
         null
        ],
        [
         "Application Started",
         "392",
         "12/18/2024",
         null
        ],
        [
         "Contacted-Referred to other AC",
         "596",
         "12/18/2024",
         null
        ],
        [
         "Grand Total",
         "30307",
         "12/18/2024",
         null
        ],
        [
         "Unable to Contact-No Response",
         "548",
         "12/18/2024",
         null
        ],
        [
         "Duplicate Record",
         "6230",
         "12/18/2024",
         null
        ],
        [
         "Contacted-Message Left",
         "12781",
         "12/18/2024",
         null
        ],
        [
         "Contacted-Interested",
         "1946",
         "12/18/2024",
         null
        ],
        [
         "Contacted-Interview Scheduled",
         "872",
         "12/18/2024",
         null
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 3724
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Count</th>\n",
       "      <th>week</th>\n",
       "      <th>Region</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Application Type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Application Completed</th>\n",
       "      <td>869</td>\n",
       "      <td>12/18/2024</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Contacted-Not Eligible</th>\n",
       "      <td>878</td>\n",
       "      <td>12/18/2024</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Contacted-Message Left</th>\n",
       "      <td>15280</td>\n",
       "      <td>12/18/2024</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Contacted-Not Interested</th>\n",
       "      <td>494</td>\n",
       "      <td>12/18/2024</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Contacted-Interview Scheduled</th>\n",
       "      <td>2679</td>\n",
       "      <td>12/18/2024</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unable to Contact-Invalid Data</th>\n",
       "      <td>43</td>\n",
       "      <td>4/9/2025</td>\n",
       "      <td>San Francisco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Contacted-Message Left</th>\n",
       "      <td>6723</td>\n",
       "      <td>4/9/2025</td>\n",
       "      <td>San Francisco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Contacted-Interview Scheduled</th>\n",
       "      <td>278</td>\n",
       "      <td>4/9/2025</td>\n",
       "      <td>San Francisco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Contacted-Referred to other AC</th>\n",
       "      <td>27</td>\n",
       "      <td>4/9/2025</td>\n",
       "      <td>San Francisco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Contacted-Not Interested</th>\n",
       "      <td>172</td>\n",
       "      <td>4/9/2025</td>\n",
       "      <td>San Francisco</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3724 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Count        week          Region\n",
       "Application Type                                                 \n",
       "Application Completed             869  12/18/2024             NaN\n",
       "Contacted-Not Eligible            878  12/18/2024             NaN\n",
       "Contacted-Message Left          15280  12/18/2024             NaN\n",
       "Contacted-Not Interested          494  12/18/2024             NaN\n",
       "Contacted-Interview Scheduled    2679  12/18/2024             NaN\n",
       "...                               ...         ...             ...\n",
       "Unable to Contact-Invalid Data     43    4/9/2025  San Francisco \n",
       "Contacted-Message Left           6723    4/9/2025  San Francisco \n",
       "Contacted-Interview Scheduled     278    4/9/2025  San Francisco \n",
       "Contacted-Referred to other AC     27    4/9/2025  San Francisco \n",
       "Contacted-Not Interested          172    4/9/2025  San Francisco \n",
       "\n",
       "[3724 rows x 3 columns]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = 'onlineprospect'\n",
    "online_prospect_files= collect_files(folder = folder, pattern = pattern)\n",
    "online_prospect_files\n",
    "for date, file in online_prospect_files.items():\n",
    "    Online_prospect_py(file , date)\n",
    "    Online_prospect_week(file, date)\n",
    "\n",
    "\n",
    "online_py_sheet = sheet_dataset(\"ONLINE_PROSPECT_PY\")\n",
    "online_prospect_sheet = sheet_dataset(\"ONLINE_PROSPECT\")\n",
    "\n",
    "\n",
    "\n",
    "with pd.ExcelWriter('online_prospect_dataset.xlsx', engine='openpyxl') as writer:\n",
    "    online_py_sheet.to_excel(writer, sheet_name=\"Online Prospect PY\", index = False)\n",
    "    online_prospect_sheet.to_excel(writer, sheet_name = \"Online Prospect\", index= False)\n",
    "df = pd.read_excel('online_prospect_dataset.xlsx', index_col=0)\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244101bb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad71d8b2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "611eafbc",
   "metadata": {},
   "source": [
    "# 4. PFD PDOF UA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b40107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional libraries for file\n",
    "from datetime import date, datetime, timedelta\n",
    "# methods for cleaning and transforming\n",
    "def PFD(week, file):\n",
    "    sheet_name = 'PFD'\n",
    "    # issue is that columns are string need to be in MM-DD-YYYY format\n",
    "    string_with_dates = {}\n",
    "    day_names = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
    "    try :\n",
    "        df = pd.read_excel(file , sheet_name = sheet_name)\n",
    "    except ValueError as e:\n",
    "        print(f\"An error occurred: {week}\")\n",
    "    # luckly the first column has start date and end date to convert string to numeric\n",
    "    date_range = df.columns[0]\n",
    "    index_string = date_range.index('between')\n",
    "    # the seperator is the between so lets get the dates\n",
    "    index_string =date_range[(index_string+len('between')):]\n",
    "    index_string = index_string.strip().replace(' ', '')\n",
    "    dates = index_string.split('and',1)\n",
    "    # Define the start and end dates\n",
    "    start_date = datetime.strptime(dates[0], '%m/%d/%Y')\n",
    "    end_date = datetime.strptime(dates[1], '%m/%d/%Y')\n",
    "    # Generate a list of dates between start_date and end_date\n",
    "    date_list = []\n",
    "    current_date = start_date\n",
    "    while current_date <= end_date:\n",
    "        date_list.append(current_date.strftime('%m/%d/%Y'))\n",
    "        current_date += timedelta(days=1)\n",
    "    for num_date in date_list:\n",
    "        # This part here gets dictionary with string as key and numeric as value\n",
    "        date_object = datetime.strptime(num_date, '%m/%d/%Y')\n",
    "        day_of_week = date_object.strftime('%A')\n",
    "        string_with_dates[day_of_week] = num_date\n",
    "    # then we clean the dataframe\n",
    "    df = df[12:]\n",
    "    df.columns = df.iloc[0]\n",
    "    df = df.reset_index().drop(columns=['index'])\n",
    "    df = df[1:]\n",
    "    need_columns = ['Region', 'Center',  \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
    "    df = df[need_columns]\n",
    "    # we replace the string to numeric version\n",
    "    for date_string, date_numeric in string_with_dates.items():\n",
    "        df = df.rename(columns={date_string : date_numeric})\n",
    "    # then we unpivot the data \n",
    "    # REGION | CENTER | DATE | VALUE\n",
    "    df = df.melt(id_vars = ['Region', 'Center'] , var_name = \"Date\",  value_name='Count')\n",
    "    df = df.sort_values(by = 'Region')\n",
    "    # insert into the apprioate file\n",
    "    start_date = week\n",
    "    sheet_name =  start_date.replace(\"/\", \"_\") + \"_PFD_report\" + \".xlsx\"\n",
    "    file_path = os.path.join(\"PFD\", sheet_name)\n",
    " \n",
    "    df.to_excel(file_path, index=False)\n",
    "\n",
    "def PDOF(week, file):\n",
    "    sheet_name = 'PDOF'\n",
    "    string_with_dates = {}\n",
    "    day_names = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
    "    try :\n",
    "        df = pd.read_excel(file , sheet_name = sheet_name)\n",
    "    except ValueError as e:\n",
    "        print(f\"An error occurred: {week}\")\n",
    "    # luckly the first column has start date and end date to convert string to numeric\n",
    "    date_range = df.columns[0]\n",
    "    index_string = date_range.index('between')\n",
    "    # the seperator is the between so lets get the dates\n",
    "    index_string =date_range[(index_string+len('between')):]\n",
    "    index_string = index_string.strip().replace(' ', '')\n",
    "    dates = index_string.split('and',1)\n",
    "    # Define the start and end dates\n",
    "    start_date = datetime.strptime(dates[0], '%m/%d/%Y')\n",
    "    end_date = datetime.strptime(dates[1], '%m/%d/%Y')\n",
    "    # Generate a list of dates between start_date and end_date\n",
    "    date_list = []\n",
    "    current_date = start_date\n",
    "    while current_date <= end_date:\n",
    "        date_list.append(current_date.strftime('%m/%d/%Y'))\n",
    "        current_date += timedelta(days=1)\n",
    "    for num_date in date_list:\n",
    "        # This part here gets dictionary with string as key and numeric as value\n",
    "        date_object = datetime.strptime(num_date, '%m/%d/%Y')\n",
    "        day_of_week = date_object.strftime('%A')\n",
    "        string_with_dates[day_of_week] = num_date\n",
    "    # then we clean the dataframe\n",
    "    df = df[12:]\n",
    "    df.columns = df.iloc[0]\n",
    "    df = df.reset_index().drop(columns=['index'])\n",
    "    df = df[1:]\n",
    "    need_columns = ['Region', 'Center',  \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
    "    df = df[need_columns]\n",
    "    # we replace the string to numeric version\n",
    "    for date_string, date_numeric in string_with_dates.items():\n",
    "        df = df.rename(columns={date_string : date_numeric})\n",
    "    # then we unpivot the data \n",
    "    # REGION | CENTER | DATE | VALUE\n",
    "    df = df.melt(id_vars = ['Region', 'Center'] , var_name = \"Date\",  value_name='Count')\n",
    "    df = df.sort_values(by = 'Region')\n",
    "    # insert into the apprioate file\n",
    "    start_date = week\n",
    "    sheet_name =  start_date.replace(\"/\", \"_\") + \"_PDOF_report\" + \".xlsx\"\n",
    "    file_path = os.path.join(\"PDOF\", sheet_name)\n",
    "    df.to_excel(file_path, index=False)\n",
    "\n",
    "def UA(week, file):\n",
    "    sheet_name = 'UA'\n",
    "    string_with_dates = {}\n",
    "    day_names = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
    "    try :\n",
    "        df = pd.read_excel(file , sheet_name = sheet_name)\n",
    "    except ValueError as e:\n",
    "        print(f\"An error occurred: {week}\")\n",
    "    # luckly the first column has start date and end date to convert string to numeric\n",
    "    date_range = df.columns[0]\n",
    "    index_string = date_range.index('between')\n",
    "    # the seperator is the between so lets get the dates\n",
    "    index_string =date_range[(index_string+len('between')):]\n",
    "    index_string = index_string.strip().replace(' ', '')\n",
    "    dates = index_string.split('and',1)\n",
    "    # Define the start and end dates\n",
    "    start_date = datetime.strptime(dates[0], '%m/%d/%Y')\n",
    "    end_date = datetime.strptime(dates[1], '%m/%d/%Y')\n",
    "    # Generate a list of dates between start_date and end_date\n",
    "    date_list = []\n",
    "    current_date = start_date\n",
    "    while current_date <= end_date:\n",
    "        date_list.append(current_date.strftime('%m/%d/%Y'))\n",
    "        current_date += timedelta(days=1)\n",
    "    for num_date in date_list:\n",
    "    # This part here gets dictionary with string as key and numeric as value\n",
    "        date_object = datetime.strptime(num_date, '%m/%d/%Y')\n",
    "        day_of_week = date_object.strftime('%A')\n",
    "        string_with_dates[day_of_week] = num_date\n",
    "    # then we clean the dataframe\n",
    "    df = df[12:]\n",
    "    df.columns = df.iloc[0]\n",
    "    df = df.reset_index().drop(columns=['index'])\n",
    "    df = df[1:]\n",
    "    need_columns = ['Region', 'Center',  \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
    "    df = df[need_columns]\n",
    "    # we replace the string to numeric version\n",
    "    for date_string, date_numeric in string_with_dates.items():\n",
    "        df = df.rename(columns={date_string : date_numeric})\n",
    "    # then we unpivot the data \n",
    "    # REGION | CENTER | DATE | VALUE\n",
    "    df = df.melt(id_vars = ['Region', 'Center'] , var_name = \"Date\",  value_name='Count')\n",
    "    df = df.sort_values(by = 'Region')\n",
    "    # insert into the apprioate file\n",
    "    start_date = week\n",
    "    sheet_name =  start_date.replace(\"/\", \"_\") + \"_UA_report\" + \".xlsx\"\n",
    "    file_path = os.path.join(\"UA\", sheet_name)\n",
    "    df.to_excel(file_path, index=False)\n",
    "def UA_separation(week, file):\n",
    "    sheet_name = \"UA Separation\"\n",
    "    try :\n",
    "        df = pd.read_excel(file , sheet_name = sheet_name)\n",
    "    except ValueError as e:\n",
    "        print(f\"An error occurred: {week}\")\n",
    "    df = df[25:]\n",
    "    path = \"1_8_Report\\PFD_PDOF_UA_Sep_01082025.xlsx\"\n",
    "    df.columns = df.iloc[0]\n",
    "    df = df.rename(columns={df.columns[2] : 'UA Sep Count'}).reset_index()\n",
    "    df = df[1:]\n",
    "    df = df.drop(columns='index')\n",
    "    wanted_cols = ['Region', 'Separation_Type', 'UA Sep Count']\n",
    "    df = df[wanted_cols]\n",
    "    # transpose first which will delelte the rows and then transpose\n",
    "    df = df.T.drop_duplicates().T\n",
    "    df['week'] = week\n",
    "    start_date = week\n",
    "    sheet_name =  start_date.replace(\"/\", \"_\") + \"_UA_SEPARATION_report\" + \".xlsx\"\n",
    "    file_path = os.path.join(\"UA_SEPARATION\", sheet_name)\n",
    "    df.to_excel(file_path, index=False)\n",
    "# gather all the PFD files\n",
    "files_pfd = collect_files(folder = folder, pattern = 'pfd_pdof_ua')\n",
    "for week, file in files_pfd.items():\n",
    "    PFD(week, file)\n",
    "    PDOF(week, file)\n",
    "    UA(week, file)\n",
    "    UA_separation(week, file)\n",
    "# merge all the files\n",
    "merge(folder = \"PFD\")\n",
    "merge(folder = \"PDOF\")\n",
    "merge(folder = \"UA\")\n",
    "merge(folder = \"UA_SEPARATION\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45fa947",
   "metadata": {},
   "source": [
    "# 5. Prospect Applicant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c644368c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# methods for cleaning and transforming the data\n",
    "def Prospect_applicant_online_prospect_submissions(week, file):\n",
    "    sheet_name = \"Online Prospects\"\n",
    "    try : \n",
    "        df = pd.read_excel(file, sheet_name = sheet_name)\n",
    "    except ValueError as e:\n",
    "        print(f\"An error occurred: {week}\")\n",
    "    df['week'] = week \n",
    "    start_date = week\n",
    "    sheet_name =  start_date.replace(\"/\", \"_\") + \"_Prospect_Applicants_Online_Prospect_report\" + \".xlsx\"\n",
    "    file_path = os.path.join(\"PROSPECT_APPLICANTS_ONLINE_PROSPECT\", sheet_name)\n",
    "    df.to_excel(file_path, index=False)\n",
    "def Prospect_applicant_oasis(week, file):\n",
    "    sheet_name = \"Applicants\"\n",
    "    try : \n",
    "        df = pd.read_excel(file, sheet_name = sheet_name)\n",
    "    except ValueError as e:\n",
    "        print(f\"An error occurred: {week}\")\n",
    "    df['week'] = week \n",
    "    start_date = week\n",
    "    sheet_name =  start_date.replace(\"/\", \"_\") + \"_Prospect_Applicants_OASIS_report\" + \".xlsx\"\n",
    "    file_path = os.path.join(\"PROSPECT_APPLICANTS_OASIS\", sheet_name)\n",
    "    df.to_excel(file_path, index=False)\n",
    "#  gather the prospect applicant  files\n",
    "files_pa = collect_files(folder=folder, pattern=\"prospects_applicants\")\n",
    "for week, file in files_pa.items():\n",
    "    Prospect_applicant_oasis(week, file)\n",
    "    Prospect_applicant_online_prospect_submissions(week,file)\n",
    "\n",
    "# merge\n",
    "merge('PROSPECT_APPLICANTS_OASIS')\n",
    "merge('PROSPECT_APPLICANTS_ONLINE_PROSPECT')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2243faa",
   "metadata": {},
   "source": [
    "# 6. COVID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23283752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# methods for cleaning and transforming the data\n",
    "def Covid(week, file):\n",
    "    sheet_name = \"Summary\"\n",
    "    try : \n",
    "        df = pd.read_excel (file, sheet_name = sheet_name)\n",
    "    except ValueError as e:\n",
    "        print(f\"An error occurred: {week}\")\n",
    "    df = df[4:]\n",
    "    df.columns = df.iloc[0]\n",
    "    df = df[0:13]\n",
    "    # want to include the total since Total is part of region and it has at least one value \n",
    "    df = df.dropna(thresh=2)\n",
    "    # loop\n",
    "    df= df.loc[:, df.columns.notna()]\n",
    "    df.columns = df.columns.str.replace(\"\\n\", \" \", regex=False)\n",
    "    df = df[1:]\n",
    "    df  =df.drop(columns = \"TOTALS\")\n",
    "    df['week'] = week \n",
    "    start_date = week\n",
    "    sheet_name =  start_date.replace(\"/\", \"_\") + \"_COVID_report\" + \".xlsx\"\n",
    "    file_path = os.path.join(\"COVID\", sheet_name)\n",
    "    df.to_excel(file_path, index=False)\n",
    "#  gather the covid files\n",
    "files_covid = collect_files(folder= folder, pattern = \"sir_covid\")\n",
    "for week, file in files_covid.items():\n",
    "    Covid(week, file)\n",
    "# # merge all the files into one\n",
    "merge(\"COVID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0541fa4f",
   "metadata": {},
   "source": [
    "# 7. Unpaid Students"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb41d1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# methods for cleaning and transforming the data\n",
    "def Unpaid_list(week, file):\n",
    "    sheet_name = \"Student List\"\n",
    "    try : \n",
    "        df = pd.read_excel(file, sheet_name = sheet_name)\n",
    "    except ValueError as e:\n",
    "        print(f\"An error occurred: {week}\")\n",
    "    df = df[0:]\n",
    "    df.columns = df.iloc[0]\n",
    "    df = df[1:]\n",
    "    df['week'] = week \n",
    "    start_date = week\n",
    "    sheet_name =  start_date.replace(\"/\", \"_\") + \"_Unpaid_List_report\" + \".xlsx\"\n",
    "    file_path = os.path.join(\"UNPAID_LIST\", sheet_name)\n",
    "    df.to_excel(file_path, index=False)\n",
    "def Unpaid_summary(week, file):\n",
    "    try : \n",
    "        sheet_name = \"Unpaid Summary by Pay Period\"\n",
    "        df = pd.read_excel(file, sheet_name=sheet_name)\n",
    "    except ValueError as e:\n",
    "        print(f\"An error occurred: {week}\")\n",
    "    df = df[3:]\n",
    "    df.columns  = df.iloc[0]\n",
    "    df = df.reset_index()\n",
    "    df = df[1:]\n",
    "    df = df.drop(columns='index')\n",
    "    df['week'] = week\n",
    "    start_date = week\n",
    "    sheet_name =  start_date.replace(\"/\", \"_\") + \"_Unpaid_Summary_report\" + \".xlsx\"\n",
    "    file_path = os.path.join(\"UNPAID_SUMMARY\", sheet_name)\n",
    "    df.to_excel(file_path, index=False)\n",
    "\n",
    "# gather the unpaid student files\n",
    "files_unpaid  = collect_files(folder=folder, pattern=\"unpaid_students\")\n",
    "for week , file in files_unpaid.items():\n",
    "    Unpaid_summary(week, file)\n",
    "    Unpaid_list(week, file)\n",
    "merge(\"UNPAID_LIST\")\n",
    "merge(\"UNPAID_SUMMARY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b54412",
   "metadata": {},
   "source": [
    "# 8. Weekly OBS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89f5d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# methods for cleaning and transforming the data\n",
    "def OBS_report(week, file):\n",
    "    try :\n",
    "        sheet_name = \"OBS Report\"\n",
    "    except ValueError as e:\n",
    "            print(f\"An error occurred: {week}\")\n",
    "    df = pd.read_excel(file, sheet_name = sheet_name)\n",
    "    df.columns = df.iloc[0]\n",
    "    df = df[1:]\n",
    "    df = df.rename(columns = {df.columns[4]: \"ACTUAL OBS\"})\n",
    "    need_columns = ['State', 'Region', 'Center', 'PY 2024 Planned OBS', 'ACTUAL OBS']\n",
    "    df =df[need_columns]\n",
    "    df['week'] = week\n",
    "    start_date = week\n",
    "    sheet_name =  start_date.replace(\"/\", \"_\") + \"_OBS_report\" + \".xlsx\"\n",
    "    file_path = os.path.join(\"OBS\", sheet_name)\n",
    "    df.to_excel(file_path, index=False)\n",
    "# gather the weekly obs files\n",
    "files_OBS = collect_files(folder = folder, pattern = 'weekly_obs')\n",
    "for week, file in files_OBS.items():\n",
    "     OBS_report(week, file)\n",
    "# merge all \n",
    "merge(folder = \"OBS\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7be1bf",
   "metadata": {},
   "source": [
    "# 9. Merge every section\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e0034a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using list comprehension\n",
    "folder = \"C:\\\\Users\\\\Andre\\\\Desktop\\\\update_weekly\"\n",
    "# get all the merged data into folder for all reports\n",
    "folder = \"C:\\\\Users\\\\Andre\\\\Desktop\\\\update_weekly\"\n",
    "common_pattern = \"_DATA\"\n",
    "report_folder = \"ALL_REPORTS\"\n",
    "data_sources = []\n",
    "\n",
    "for secondary_dir in [d for d in os.listdir(folder) if d.isupper()]:\n",
    "    secondary_dir_path = os.path.join(folder, secondary_dir)\n",
    "    files_to_process = [f for f in os.listdir(secondary_dir_path) if common_pattern in f]\n",
    "\n",
    "    # Process each file that matches the pattern\n",
    "    for file in files_to_process:\n",
    "        file_path = os.path.join(secondary_dir_path, file)\n",
    "        df = pd.read_excel(file_path)\n",
    "        df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "        final_path = os.path.join(report_folder, file)\n",
    "        df.to_excel(final_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854ab527",
   "metadata": {},
   "source": [
    "# 10 Testing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69afcd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking for data that has center\n",
    "pattern = 'center'\n",
    "path = \"ALL_REPORTS\"\n",
    "has_centers = {}\n",
    "import os \n",
    "import pandas as pd\n",
    "\n",
    "for file in os.listdir(path):\n",
    "    fp = os.path.join(path, file)\n",
    "    df = pd.read_excel(fp)\n",
    "    # just to loop through\n",
    "    pattern_not_found = False \n",
    "    for column in df.columns:\n",
    "        if pattern ==  column.lower() or 'center code' == column.lower():\n",
    "            pattern_not_found = True\n",
    "    if pattern_not_found is True : \n",
    "        has_centers[file] = True\n",
    "    else:\n",
    "        has_centers[file] = False\n",
    "        \n",
    "true_keys = {key: value for key, value in has_centers.items() if value is True}\n",
    "print(true_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ba24c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"1_1_Report\\Arrivals_Separations_12-26-2024_through_01-01-2025.xlsx\"\n",
    "df_2 = pd.read_excel(path, sheet_name='Arrivals')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5747978b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "path = \"ALL_REPORTS\\ARRIVALS_DATA.xlsx\"\n",
    "center = 'Grafton'\n",
    "df = pd.read_excel(path,index_col=0)\n",
    "\n",
    "# filter\n",
    "df = df[df['Center Name'].str.contains(center)]\n",
    "\n",
    "start_df = df[df['week']==df['week'].min()]\n",
    "\n",
    "len(start_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3b9078",
   "metadata": {},
   "outputs": [],
   "source": [
    "og_file = \"1_1_Report\\Arrivals_Separations_12-26-2024_through_01-01-2025.xlsx\"\n",
    "in_folder = \"ARRIVALS\\\\1_1_2025_for_arrivals_report.xlsx\"\n",
    "merge_file = \"ALL_REPORTS\\ARRIVALS_DATA.xlsx\"\n",
    "df_og = pd.read_excel(og_file, sheet_name=\"Arrivals\")\n",
    "df_folder = pd.read_excel(in_folder)\n",
    "len(df_folder) == len(df_og)\n",
    "df_merge = pd.read_excel(merge_file)\n",
    "len(df_merge[df_merge['week']==df_merge['week'].min()]) == len(df_folder) == len(df_og)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62a1504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at streamlit and comparing\n",
    "df_merge = pd.read_excel(merge_file, index_col=0)\n",
    "center = 'Grafton'\n",
    "data = df_merge[df_merge['Center Name'].str.contains(center)]\n",
    "\n",
    "data['week'] = pd.to_datetime(data['week']).dt.date\n",
    "# data.melt(id_vars=['Center Name', 'week'])\n",
    "id_vars = ['Center Name', 'week']\n",
    "include= ['Center Name', 'week', 'Total Arrivals', 'Online Arrivals', 'Scout Arrivals', 'Other Arrivals']\n",
    "data =data[include]\n",
    "\n",
    "data = data.melt(id_vars=id_vars)\n",
    "sum_per_group = data.groupby(['week', 'variable'])['value'].sum().reset_index()\n",
    "\n",
    "pivot_table =sum_per_group.pivot(index='week', columns='variable', values='value').fillna(0)\n",
    "\n",
    "\n",
    "st.bar_chart(pivot_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
